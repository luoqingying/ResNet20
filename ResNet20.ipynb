{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train  = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding = 4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # range 0 to 1\n",
    "        transforms.ToTensor(), \n",
    "        # ref mean and std\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet20 Architectrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsize=None):\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        # 32 -> 16, 16->8 need stride=2\n",
    "        self.stride = stride\n",
    "        \n",
    "        # each block contains 2 3x3 conv\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        # second filter always use stride=1 to keep orginal size\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        self.downsize = downsize # a function used to make the size conform\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsize is not None:\n",
    "            # resize the volume that goes through the skip connection\n",
    "            residual = self.downsize(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.in_channel = 16\n",
    "        self.downsize = None\n",
    "        \n",
    "        # very first layer, change [128 3 32 32] to [128 16 32 32]\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # input image: 32 x 32, then 3x3x16 conv with stride = 1 and pad = 1, so we have 32x32x16 in the first block\n",
    "        self.layer1 = self.build_layer(block, 16, 3, stride=1)\n",
    "        self.layer2 = self.build_layer(block, 32, 3, stride=2)\n",
    "        self.layer3 = self.build_layer(block, 64, 3, stride=2)\n",
    "        \n",
    "        self.linear = nn.Linear(64, 10)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_() \n",
    "        \n",
    "    def build_layer(self, block, out_channel, num_block=3, stride=1):\n",
    "        if stride != 1 or self.in_channel != out_channel:\n",
    "            self.downsize = nn.Sequential(\n",
    "                # use 1x1 conv to do the size matching\n",
    "                nn.Conv2d(self.in_channel, out_channel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "            )\n",
    "        else:\n",
    "            self.downsize = None\n",
    "        # should have 3 layers\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, out_channel, stride, self.downsize))\n",
    "        self.in_channel = out_channel\n",
    "        # only the first block need to downsize\n",
    "        for i in range(1, num_block):\n",
    "            layers.append(block(self.in_channel, out_channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 32x32x3 -> 32x32x16\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)     \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)   # 32x32x16\n",
    "        out = self.layer2(out)   # 16x16x32\n",
    "        out = self.layer3(out)   # 8x8x64\n",
    "\n",
    "        out = F.avg_pool2d(input=out, kernel_size=out.size(3))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ResNet(Block)\n",
    "net = net.to(device)\n",
    "\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-4\n",
    "EPOCHS = 100\n",
    "DECAY_EPOCHS = 2\n",
    "DECAY = 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=current_learning_rate, momentum=MOMENTUM, weight_decay=REG, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch ...\n",
      "Starting from learning rate 0.100000:\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"./saved_model\"\n",
    "\n",
    "TRAIN_FROM_SCRATCH = True\n",
    "\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Train the training dataset for 1 epoch.\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_examples += predicted.size(0) # 128 for each batch\n",
    "        correct_examples += predicted.eq(targets).sum().item()\n",
    "        train_loss += loss\n",
    "        global_step += 1\n",
    "                \n",
    "    avg_loss = train_loss / (batch_idx + 1)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    \n",
    "    train_loss_list.append(avg_loss)\n",
    "    train_acc_list.append(avg_acc)\n",
    "\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    # validate for each epoch\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_examples += predicted.size(0)\n",
    "            correct_examples += predicted.eq(targets).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    \n",
    "    val_loss_list.append(avg_loss)\n",
    "    val_acc_list.append(avg_acc)\n",
    "    \n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    # Handle the learning rate scheduler.\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate * DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
